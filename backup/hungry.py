from openai import OpenAI  # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ OpenAI í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸
import openai  # ìµœì‹  OpenAI ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
import streamlit as st  # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os  # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ì„ ìœ„í•œ os ëª¨ë“ˆ ì„í¬íŠ¸
import base64  # ì´ë¯¸ì§€ ì¸ì½”ë”©ì„ ìœ„í•œ base64 ëª¨ë“ˆ ì„í¬íŠ¸
import json  # JSON íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ json ëª¨ë“ˆ ì„í¬íŠ¸
from dotenv import load_dotenv  # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì„í¬íŠ¸
from pinecone import Pinecone, ServerlessSpec  # ìµœì‹  Pinecone API ì‚¬ìš©
import hashlib  # íŒŒì¼ í•´ì‹œ ê³„ì‚°ì„ ìœ„í•œ ëª¨ë“ˆ
import io  # ë©”ëª¨ë¦¬ ë‚´ íŒŒì¼ ê°ì²´ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
from gtts import gTTS  # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ gTTS ë¼ì´ë¸ŒëŸ¬ë¦¬

# ==========================================
# ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================
load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
openai.api_key = os.getenv("OPENAI_API_KEY")

st.set_page_config(page_title="KCC Auto Manager ğŸš—", layout="wide")
st.title("KCC Auto Manager ğŸš—")
#st.caption("ìë™ì°¨ ì‚¬ìš© ë§¤ë‰´ì–¼ ë° ì„œë¹„ìŠ¤ ì„¼í„° ìœ„ì¹˜ ì°¾ê¸°ë¥¼ ë„ì™€ë“œë¦½ë‹ˆë‹¤!")

# ê¸°ë³¸ í—¤ë”, í‘¸í„°, ë©”ë‰´ ìˆ¨ê¸°ê¸°
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;} /* ì¢Œì¸¡ ìƒë‹¨ í–„ë²„ê±° ë©”ë‰´ */
    header {visibility: hidden;}    /* ìƒë‹¨ í—¤ë” */
    footer {visibility: hidden;}    /* í•˜ë‹¨ í‘¸í„° */
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)


# session_state ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "input_counter" not in st.session_state:
    st.session_state.input_counter = 0  # ê° ì§ˆë¬¸ë§ˆë‹¤ ìƒˆë¡œìš´ ìœ„ì ¯ key ìƒì„±

def clear_conversation():
    st.session_state.messages = []
    st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.title("ğŸ¤—ğŸ’¬ Auto Manager ğŸš—")
    st.subheader("í˜„ì¬ ì°¨ëŸ‰: ë²¤ì¸  S í´ë˜ìŠ¤")
    st.markdown("### ëŒ€í™” ì´ë ¥")
    for msg in st.session_state.messages[-5:]:
        role = "ë‚˜" if msg["role"] == "user" else "ì±—ë´‡"
        st.markdown(f"**{role}:** {msg['content'][:30]}...")
    st.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=clear_conversation)
    st.markdown("### ì„¤ì •")
    user_language = st.selectbox("ì‚¬ìš© ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”", ("í•œêµ­ì–´", "English", "Deutsch"))
    st.markdown(f"ì„ íƒëœ ì–¸ì–´: **{user_language}**")
    st.markdown("### ë°”ë¡œê°€ê¸°")
    st.markdown("[ë©”ë¥´ì„¸ë°ìŠ¤-ë²¤ì¸  ê³µì‹ í™ˆí˜ì´ì§€](https://www.mercedes-benz.co.kr/)")

# ==========================================
# Pinecone ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë°ì´í„° ì¸ë±ì‹± í•¨ìˆ˜
# ==========================================
def get_embedding(text):
    if not text.strip():
        raise ValueError("ì„ë² ë”©ì— ì „ë‹¬í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    response = openai.embeddings.create(input=[text], model="text-embedding-ada-002")
    return response.data[0].embedding

def get_file_hash(filename):
    h = hashlib.sha256()
    with open(filename, 'rb') as f:
        h.update(f.read())
    return h.hexdigest()

def load_stored_hash(hash_file):
    try:
        with open(hash_file, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        return None

def save_hash(hash_file, hash_value):
    with open(hash_file, "w", encoding="utf-8") as f:
        f.write(hash_value)

def index_data():
    hash_file = "tes.json.hash"
    current_hash = get_file_hash("test.json")
    stored_hash = load_stored_hash(hash_file)
    
    if stored_hash != current_hash:
        with open("test.json", "r", encoding="utf-8") as f:
            test_data = json.load(f)
        vectors = []
        for item in test_data:
            pdf_file = item.get("pdf_file", "")
            structure = item.get("structure", [])
            for i, section in enumerate(structure):
                title = section.get("title", "")
                sub_titles = section.get("sub_titles", [])
                content_text = title
                image_paths = []
                for sub in sub_titles:
                    sub_title = sub.get("title", "")
                    contents = sub.get("contents", [])
                    for content in contents:
                        if content.lower().endswith(('.jpeg', '.jpg', '.png', '.gif')):
                            image_paths.append(content)
                    non_image_contents = [c for c in contents if not c.lower().endswith(('.jpeg', '.jpg', '.png', '.gif'))]
                    content_text += "\n" + sub_title + "\n" + "\n".join(non_image_contents)
                doc_id = f"{pdf_file}_{i}"
                embedding = get_embedding(content_text)
                metadata = {
                    "pdf_file": pdf_file,
                    "section_title": title,
                    "content": content_text,
                    "image_paths": image_paths
                }
                vectors.append((doc_id, embedding, metadata))
        index.upsert(vectors=vectors)
        save_hash(hash_file, current_hash)
#        st.write("Pineconeì— ìƒˆ ë°ì´í„°ë¥¼ ì¸ë±ì‹±í–ˆìŠµë‹ˆë‹¤.")
#    else:
#        st.write("test.json íŒŒì¼ì— ë³€ê²½ì´ ì—†ìœ¼ë¯€ë¡œ, ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

pinecone_api_key = os.getenv("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)
index_name = "kcc-llm"
if index_name not in [idx.name for idx in pc.list_indexes()]:
    pc.create_index(
        name=index_name,
        dimension=1536,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
index = pc.Index(index_name)

with st.spinner("ë°ì´í„° ì¸ë±ì‹± ì¤‘..."):
    index_data()

# ==========================================
# Pinecone ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜
# ==========================================
def get_pinecone_context(query_text):
    if not query_text.strip():
        return "", None

    query_embedding = get_embedding(query_text)
    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)
    pinecone_context = ""
    displayed_image = None
    for match in results["matches"]:
        metadata = match["metadata"]
        pinecone_context += (
            f"Section: {metadata.get('section_title', '')}\n"
            f"Content: {metadata.get('content', '')}\n\n"
        )
        if metadata.get("image_paths"):
            displayed_image = metadata["image_paths"]
            if isinstance(displayed_image, str):
                if displayed_image.strip() in ("", "[]"):
                    displayed_image = None
                else:
                    try:
                        parsed = json.loads(displayed_image)
                        if isinstance(parsed, list) and parsed:
                            displayed_image = parsed[0]
                        else:
                            displayed_image = displayed_image
                    except Exception:
                        pass
            elif isinstance(displayed_image, list):
                if displayed_image:
                    displayed_image = displayed_image[0]
                else:
                    displayed_image = None
    return pinecone_context, displayed_image

# ==========================================
# ChatGPT ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í•¨ìˆ˜ (ìŠ¤í”¼ë„ˆ í¬í•¨)
# ==========================================
def ask_chatgpt_stream(question, pinecone_context):
    try:
        system_message = (
            "ë„ˆëŠ” ë²¤ì¸  S-class ì‚¬ìš© ë§¤ë‰´ì–¼ì— ëŒ€í•´ ì „ë¬¸ê°€ì•¼. "
            "ì•„ë˜ëŠ” ê´€ë ¨ ë§¤ë‰´ì–¼ ì •ë³´ì…ë‹ˆë‹¤:\n" + pinecone_context
        )
        conversation = [{"role": "system", "content": system_message}]
        conversation.extend(st.session_state.messages)
        conversation.append({"role": "user", "content": question})
        
        full_response = ""
        with st.spinner("ì±—ë´‡ ì‘ë‹µ ì¤‘..."):
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=conversation,
                stream=True
            )
            for chunk in response:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
        return full_response

    except Exception as e:
        st.error(f"Error: {str(e)}")
        return ""

# ==========================================
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (í…ìŠ¤íŠ¸, ìŒì„±, ì´ë¯¸ì§€)
# ==========================================
current_key = st.session_state.input_counter

user_prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key=f"user_prompt_{current_key}")

with st.expander("ìŒì„± ì…ë ¥ ë° ì´ë¯¸ì§€ ì²¨ë¶€ ì—´ê¸°"):
    audio_file = st.audio_input("ìŒì„±ì„ ë…¹ìŒí•˜ì„¸ìš”", key=f"audio_file_{current_key}")
    uploaded_image = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg", "gif"], key=f"uploaded_image_{current_key}")

if audio_file is not None:
    with st.spinner("ìŒì„± ì¸ì‹ ì¤‘..."):
        transcript_result = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language="ko"
        )
    user_prompt = transcript_result.text

if user_prompt or uploaded_image:
    combined_prompt = user_prompt or ""
    if uploaded_image:
        st.image(uploaded_image, width=150, caption="ì²¨ë¶€ëœ ì´ë¯¸ì§€")
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥ (í…ìŠ¤íŠ¸ë§Œ)
    st.session_state.messages.append({"role": "user", "content": combined_prompt})
    
    if combined_prompt.strip():
        pinecone_context, related_image = get_pinecone_context(combined_prompt)
    else:
        pinecone_context, related_image = "", None

    # ì±—ë´‡ ì‘ë‹µ ìƒì„±
    assistant_response = ask_chatgpt_stream(combined_prompt, pinecone_context)
    
    # TTS ì²˜ë¦¬: ì‘ë‹µì´ ìˆì„ ë•Œë§Œ ì§„í–‰
    if assistant_response.strip():
        try:
            tts = gTTS(assistant_response, lang='ko')
            audio_fp = io.BytesIO()
            tts.write_to_fp(audio_fp)
            audio_fp.seek(0)
            tts_audio_bytes = audio_fp.getvalue()
            # base64 ì¸ì½”ë”©í•˜ì—¬ ì €ì¥ (JSON ì§ë ¬í™” ë¬¸ì œ í•´ê²°)
            tts_audio_b64 = base64.b64encode(tts_audio_bytes).decode("utf-8")
        except Exception as tts_error:
            st.error(f"TTS ì—ëŸ¬: {tts_error}")
            tts_audio_b64 = None
    else:
        tts_audio_b64 = None

    # ì±—ë´‡ ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥ (TTS ë° ê´€ë ¨ ì´ë¯¸ì§€ í¬í•¨)
    assistant_message = {
        "role": "assistant",
        "content": assistant_response,
        "tts": tts_audio_b64,
        "image": related_image  # ê´€ë ¨ ì´ë¯¸ì§€ ì €ì¥
    }
    st.session_state.messages.append(assistant_message)
    
    # ì²˜ë¦¬ í›„ ì¹´ìš´í„° ì¦ê°€ (ë‹¤ìŒ ì§ˆë¬¸ ì‹œ ìƒˆë¡œìš´ ìœ„ì ¯ key ì‚¬ìš©)
    st.session_state.input_counter += 1

# ==========================================
# ì „ì²´ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ (ìµœì¢… ì—…ë°ì´íŠ¸)
# ==========================================
st.markdown("### ëŒ€í™” ê¸°ë¡")
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # assistant ë©”ì‹œì§€: ê´€ë ¨ ì´ë¯¸ì§€ë¥¼ ë¨¼ì € í‘œì‹œ (ë‹µë³€ ë°”ë¡œ ìœ„ì—)
        if message["role"] == "assistant" and message.get("image"):
            st.image(message["image"], caption="ê´€ë ¨ ì´ë¯¸ì§€", use_container_width=True)
        st.markdown(message["content"])
        # assistant ë©”ì‹œì§€ì— TTSê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ base64 ë””ì½”ë”© í›„ ì˜¤ë””ì˜¤ ì¶œë ¥
        if message["role"] == "assistant" and message.get("tts"):
            audio_bytes = base64.b64decode(message["tts"])
            st.audio(audio_bytes, format="audio/mp3")
