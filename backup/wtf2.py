from openai import OpenAI  # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ OpenAI í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸
import openai  # ìµœì‹  OpenAI ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
import streamlit as st  # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os  # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ì„ ìœ„í•œ os ëª¨ë“ˆ ì„í¬íŠ¸
import base64  # ì´ë¯¸ì§€ ì¸ì½”ë”©ì„ ìœ„í•œ base64 ëª¨ë“ˆ ì„í¬íŠ¸
import json  # JSON íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ json ëª¨ë“ˆ ì„í¬íŠ¸
from dotenv import load_dotenv  # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì„í¬íŠ¸
from pinecone import Pinecone, ServerlessSpec  # ìµœì‹  Pinecone API ì‚¬ìš©
import hashlib  # íŒŒì¼ í•´ì‹œ ê³„ì‚°ì„ ìœ„í•œ ëª¨ë“ˆ
import io  # ë©”ëª¨ë¦¬ ë‚´ íŒŒì¼ ê°ì²´ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
from gtts import gTTS  # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ gTTS ë¼ì´ë¸ŒëŸ¬ë¦¬

# .env íŒŒì¼ì— ì €ì¥ëœ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° API í‚¤ ì„¤ì •
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
openai.api_key = os.getenv("OPENAI_API_KEY")

# ë©”ì¸ íƒ€ì´í‹€ ë° ìº¡ì…˜ ì„¤ì •
st.title("KCC Auto Manager ğŸš—")
st.caption("ìë™ì°¨ ì‚¬ìš© ë§¤ë‰´ì–¼ ë° ì„œë¹„ìŠ¤ ì„¼í„° ìœ„ì¹˜ ì°¾ê¸°ë¥¼ ë„ì™€ë“œë¦½ë‹ˆë‹¤!")

# ì‚¬ìš©ì ì…ë ¥ í•„ë“œ ë° ë²„íŠ¼ ë°°ì¹˜
toggle_expander = st.session_state.get("toggle_expander", False)
col1, col2 = st.columns([0.9, 0.1])
with col1:
    prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
with col2:
    if st.button("ğŸ”—"):
        st.session_state.toggle_expander = not toggle_expander

# ìŒì„± ì…ë ¥ ë° ì´ë¯¸ì§€ ì—…ë¡œë“œ ì˜ì—­
toggle_expander = st.session_state.get("toggle_expander", False)
if toggle_expander:
    st.markdown("**ìŒì„± ì…ë ¥ ë° ì´ë¯¸ì§€ ì²¨ë¶€**")
    audio_file = st.audio_input("ìŒì„±ì„ ë…¹ìŒí•˜ì„¸ìš”.")
    uploaded_image = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg", "gif"])
else:
    audio_file = None
    uploaded_image = None

##########################################
# ChatGPT ì‘ë‹µ í•¨ìˆ˜ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
##########################################
def ask_chatgpt_stream(question):
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": question}],
            stream=True
        )
        answer_container = st.empty()
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                full_response += chunk.choices[0].delta.content
                answer_container.markdown(full_response)
        return full_response
    except Exception as e:
        st.error(f"Error: {str(e)}")
        return ""

##########################################
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
##########################################
if audio_file is not None:
    transcript_result = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio_file,
        language="ko"
    )
    prompt = transcript_result.text

if prompt or uploaded_image:
    combined_prompt = prompt or ""
    st.chat_message("user").markdown(combined_prompt)
    
    if uploaded_image is not None:
        st.image(uploaded_image, width=150, caption="ì²¨ë¶€ëœ ì´ë¯¸ì§€")
    
    response = ask_chatgpt_stream(combined_prompt)
    
    with st.chat_message("assistant"):
        st.markdown(response)
        
        try:
            tts = gTTS(response, lang='ko')
            audio_fp = io.BytesIO()
            tts.write_to_fp(audio_fp)
            audio_fp.seek(0)
            st.audio(audio_fp, format="audio/mp3")
        except Exception as tts_error:
            st.error(f"TTS ì—ëŸ¬: {tts_error}")
