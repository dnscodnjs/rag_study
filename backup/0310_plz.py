from openai import OpenAI  # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ OpenAI í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸
import streamlit as st
import os
import base64
from dotenv import load_dotenv
import speech_recognition as sr
import io

# ìŒì„± ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ì—ì„œ audiorecorder í•¨ìˆ˜ë§Œ ì„í¬íŠ¸
from streamlit_audiorecorder import audiorecorder

load_dotenv()  # .env íŒŒì¼ì— ì €ì¥ëœ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY")
)

st.title("KCC Auto Manager ğŸš—")
st.caption("ìë™ì°¨ ì‚¬ìš© ë§¤ë‰´ì–¼ ë° ì„œë¹„ìŠ¤ ì„¼í„° ìœ„ì¹˜ ì°¾ê¸°ë¥¼ ë„ì™€ë“œë¦½ë‹ˆë‹¤!")

with st.sidebar:
    st.title('ğŸ¤—ğŸ’¬ Auto Manager ğŸš—')

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])


def ask_chatgpt_stream(question):
    try:
        conversation = [{"role": "system", "content": "ë„ˆëŠ” ë²¤ì¸  S-class ì‚¬ìš© ë©”ë‰´ì–¼ì— ëŒ€í•´ ì „ë¬¸ê°€ì•¼."}]
        conversation.extend(st.session_state.messages)
        conversation.append({"role": "user", "content": question})

        response = client.chat.completions.create(
            model="o3-mini",
            messages=conversation,
            stream=True
        )

        answer_container = st.empty()
        full_response = ""

        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                full_response += chunk.choices[0].delta.content
                answer_container.markdown(full_response)
        return full_response

    except Exception as e:
        st.error(f"Error: {str(e)}")
        return ""


st.subheader("ìŒì„± ì…ë ¥")
st.caption("ë…¹ìŒ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„± ì…ë ¥ í›„ ì¸ì‹ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
audio_bytes = audiorecorder()  # ìŒì„± ë…¹ìŒ ì»´í¬ë„ŒíŠ¸ í˜¸ì¶œ

voice_input = ""
if audio_bytes is not None:
    audio_file = io.BytesIO(audio_bytes)
    r = sr.Recognizer()
    try:
        with sr.AudioFile(audio_file) as source:
            audio_data = r.record(source)
        voice_input = r.recognize_google(audio_data, language='ko-KR')
        st.success("ìŒì„± ì¸ì‹ ê²°ê³¼:")
        st.write(voice_input)
    except sr.UnknownValueError:
        st.error("ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except sr.RequestError as e:
        st.error(f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ìš”ì²­ ì—ëŸ¬: {e}")

st.subheader("í…ìŠ¤íŠ¸ ì…ë ¥")
prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
uploaded_image = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì²¨ë¶€í•˜ì„¸ìš”", type=["png", "jpg", "jpeg", "gif"])

combined_prompt = ""
if prompt:
    combined_prompt += prompt
if voice_input:
    if combined_prompt:
        combined_prompt += "\n[ìŒì„± ì…ë ¥]: " + voice_input
    else:
        combined_prompt = voice_input

if combined_prompt or uploaded_image:
    st.session_state.messages.append({"role": "user", "content": combined_prompt})

    with st.chat_message("user"):
        if prompt:
            st.markdown(prompt)
        if voice_input:
            st.markdown(f"**[ìŒì„± ì…ë ¥]:** {voice_input}")
        if uploaded_image is not None:
            st.image(uploaded_image, width=150, caption="ì²¨ë¶€ëœ ì´ë¯¸ì§€")

    with st.chat_message("assistant"):
        response = ask_chatgpt_stream(combined_prompt)

    st.session_state.messages.append({"role": "assistant", "content": response})
