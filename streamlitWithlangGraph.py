from openai import OpenAI  # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ OpenAI í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸
import openai  # ìµœì‹  OpenAI ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©
import streamlit as st  # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os  # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥ì„ ìœ„í•œ os ëª¨ë“ˆ ì„í¬íŠ¸
import base64  # ì´ë¯¸ì§€ ì¸ì½”ë”©ì„ ìœ„í•œ base64 ëª¨ë“ˆ ì„í¬íŠ¸
import json  # JSON íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ json ëª¨ë“ˆ ì„í¬íŠ¸
from dotenv import load_dotenv  # .env íŒŒì¼ì˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¡œë“œí•˜ê¸° ìœ„í•œ í•¨ìˆ˜ ì„í¬íŠ¸
import hashlib  # íŒŒì¼ í•´ì‹œ ê³„ì‚°ì„ ìœ„í•œ ëª¨ë“ˆ
import io  # ë©”ëª¨ë¦¬ ë‚´ íŒŒì¼ ê°ì²´ ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
from gtts import gTTS  # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ gTTS ë¼ì´ë¸ŒëŸ¬ë¦¬

## image similarity finder
import cv2
import torch
import clip
from PIL import Image
import numpy as np
import pickle
import re

from typing import List, Optional
from typing_extensions import TypedDict

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_community.tools import TavilySearchResults
from langchain.tools import tool
from langchain_pinecone import PineconeVectorStore
from langchain.docstore.document import Document
from langchain_core.messages import HumanMessage
from langgraph.prebuilt import create_react_agent
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command
from pinecone import Pinecone  # ìµœì‹  Pinecone API ì‚¬ìš©
from typing import Literal

# ==========================================
# ì„¤ì • ë° ì´ˆê¸°í™”
# ==========================================
load_dotenv()
file_path = "./files/"
index_name = "kcc"
car_type = "EQS"

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
openai.api_key = os.getenv("OPENAI_API_KEY")
embedding = OpenAIEmbeddings(model="text-embedding-3-large") #text-embedding-ada-002, text-embedding-3-large
database = PineconeVectorStore(index_name=index_name, embedding=embedding)
retriever = database.as_retriever(search_kwargs={"k": 3})
llm = ChatOpenAI(model='gpt-4o')

class AgentState(TypedDict, total=False):
    messages: List[HumanMessage]  # ê¸°ì¡´ ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    image: Optional[str]          # ë‹¨ì¼ ì´ë¯¸ì§€ URL (ì˜ˆ: ì„ íƒëœ ì´ë¯¸ì§€)
    # í•„ìš”ì— ë”°ë¼ ì¶”ê°€ ì •ë³´ë¥¼ ìœ„í•œ í•„ë“œë¥¼ ë” ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# ==========================================
# ì´ë¯¸ì§€ ê´€ë ¨ ì„¤ì • ë° í•¨ìˆ˜ ì´ˆê¸°í™”
# ==========================================
def imread_unicode(file_path):
    stream = np.fromfile(file_path, dtype=np.uint8)
    img = cv2.imdecode(stream, cv2.IMREAD_COLOR)
    return img

def resize_image(image, max_dim=800):
    height, width = image.shape[:2]
    if max(height, width) > max_dim:
        scaling_factor = max_dim / float(max(height, width))
        new_width = int(width * scaling_factor)
        new_height = int(height * scaling_factor)
        resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)
        return resized_image
    return image

# CLIP ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ ë¡œë”© (ì˜ˆ: RN50 ì‚¬ìš©í•˜ë©´ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-L/14", device=device)
local_path = "C:/inflearn-rag-notebook-main"
input_path = local_path + "/input"
icon_folder = "C:/inflearn-rag-notebook-main/icons/ê³„ê¸°íŒ_ë””ìŠ¤í”Œë ˆì´"
embeddings_path = file_path + "icon_embeddings.pkl"
IMAGE_PATTERN = r'((?:[A-Za-z]:/|https?://)\S+\.(?:png|jpg|jpeg|gif))'

# ==========================================
# Streamlit UI êµ¬ì„±
# ==========================================
st.set_page_config(
    page_title="KCC Auto Manager", 
    page_icon= Image.open(local_path + "/images/favicon.ico"),
    layout="wide")
st.title("KCC Auto Manager")

st.markdown(
    """
    <style>
    @media (max-width: 768px) {
        .responsive-image {
            width: 85% !important;
        }
    }
    @media (min-width: 769px) {
        .responsive-image {
            width: 50% !important;
        }
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ê¸°ë³¸ í—¤ë”, í‘¸í„°, ë©”ë‰´ ìˆ¨ê¸°ê¸°
hide_streamlit_style = """
    <style>
    #MainMenu {visibility: hidden;} /* ì¢Œì¸¡ ìƒë‹¨ í–„ë²„ê±° ë©”ë‰´ */
    header {visibility: hidden;}    /* ìƒë‹¨ í—¤ë” */
    footer {visibility: hidden;}    /* í•˜ë‹¨ í‘¸í„° */
    </style>
"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)
if "messages" not in st.session_state:
    st.session_state.messages = []
if "input_counter" not in st.session_state:
    st.session_state.input_counter = 0  # ê° ì§ˆë¬¸ë§ˆë‹¤ ìƒˆë¡œìš´ ìœ„ì ¯ key ìƒì„±

def clear_conversation():
    st.session_state.messages = []
    st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.title("ğŸ¤—ğŸ’¬ Auto Manager ğŸš—")
    st.subheader("í˜„ì¬ ì°¨ëŸ‰: ë²¤ì¸  S í´ë˜ìŠ¤")
    st.markdown("### ëŒ€í™” ì´ë ¥")
    for msg in st.session_state.messages[-5:]:
        role = "ë‚˜" if msg["role"] == "user" else "ì±—ë´‡"
        st.markdown(f"**{role}:** {msg['content'][:30]}...")
    st.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=clear_conversation)
    st.markdown("### ì„¤ì •")
    user_language = st.selectbox("ì‚¬ìš© ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”", ("í•œêµ­ì–´", "English", "Deutsch"))
    st.markdown(f"ì„ íƒëœ ì–¸ì–´: **{user_language}**")
    st.markdown("### ë°”ë¡œê°€ê¸°")
    st.markdown("[ë©”ë¥´ì„¸ë°ìŠ¤-ë²¤ì¸  ê³µì‹ í™ˆí˜ì´ì§€](https://www.mercedes-benz.co.kr/)")

# ==========================================
# Pinecone ì¸ë±ìŠ¤ ì´ˆê¸°í™” ë° ë°ì´í„° ì¸ë±ì‹± í•¨ìˆ˜
# ==========================================
def get_file_hash(filename):
    h = hashlib.sha256()
    with open(filename, 'rb') as f:
        h.update(f.read())
    return h.hexdigest()
def load_stored_hash(hash_file):
    try:
        with open(hash_file, "r", encoding="utf-8") as f:
            return f.read().strip()
    except FileNotFoundError:
        return None
def save_hash(hash_file, hash_value):
    with open(hash_file, "w", encoding="utf-8") as f:
        f.write(hash_value)
def index_data():
    """
    usage.json íŒŒì¼ì˜ ë‚´ìš©ì„ ì½ì–´ì™€ ê° ì„¹ì…˜ì˜ í•˜ìœ„ í•­ëª©(ì„œë¸Œíƒ€ì´í‹€) ë‹¨ìœ„ë¡œ ë¬¸ì„œë¥¼ ìƒì„±í•˜ê³ ,
    ê° ë¬¸ì„œì—ëŠ” ì•„ë˜ì˜ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤.
      - ì „ì²´ ì„¹ì…˜ ì œëª©
      - í•´ë‹¹ ì„œë¸Œíƒ€ì´í‹€ ì œëª©
      - ì„œë¸Œíƒ€ì´í‹€ì˜ ì „ì²´ ë‚´ìš© (í…ìŠ¤íŠ¸ë¡œ ê²°í•©)
      - PDF íŒŒì¼ ì´ë¦„
      - ì„œë¸Œíƒ€ì´í‹€ì—ì„œ ì¶”ì¶œí•œ ì´ë¯¸ì§€ URL ëª©ë¡ (contentsì™€ images ëª¨ë‘ í™•ì¸)
    ì´ì „ì— ì €ì¥ëœ í•´ì‹œì™€ ë¹„êµí•˜ì—¬ ë³€ê²½ì´ ìˆì„ ë•Œë§Œ ì¸ë±ì‹±ì„ ì§„í–‰í•©ë‹ˆë‹¤.
    """
    hash_file = file_path + "test.json.hash"
    current_hash = get_file_hash(file_path + "test.json")
    stored_hash = load_stored_hash(hash_file)
    
    if stored_hash != current_hash:
        json_file = file_path + "test.json"
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
            documents = []
            # usage.jsonì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ í•­ëª©ë“¤ì„ í¬í•¨
            for item in data:
                pdf_file = item.get("pdf_file", "")
                structure = item.get("structure", [])
                # ê° ì„¹ì…˜ì— ëŒ€í•´
                for section in structure:
                    section_title = section.get("title", "")
                    sub_titles = section.get("sub_titles", [])
                    # ê° ì„œë¸Œíƒ€ì´í‹€ ë‹¨ìœ„ë¡œ Document ìƒì„±
                    for sub in sub_titles:
                        sub_title = sub.get("title", "")
                        contents = sub.get("contents", [])
                        # ì„œë¸Œíƒ€ì´í‹€ì˜ ë‚´ìš© ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
                        content_text = f"{section_title}\n{sub_title}\n" + "\n".join(contents)
                        
                        # ì´ë¯¸ì§€ URL ì¶”ì¶œ: contentsì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œì™€ subì˜ images í•­ëª© ëª¨ë‘ í™•ì¸
                        image_paths = []
                        # contents ë‚´ì˜ ì´ë¯¸ì§€ URL (íŒŒì¼ í™•ì¥ìë¡œ íŒë³„)
                        for content in contents:
                            if content.lower().endswith(('.jpeg', '.jpg', '.png', '.gif')):
                                image_paths.append(content)
                        # sub í•­ëª© ë‚´ì— images í‚¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€ (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬)
                        images = sub.get("images", [])
                        if images:
                            if isinstance(images, list):
                                image_paths.extend(images)
                            elif isinstance(images, str):
                                image_paths.append(images)
                        
                        # ë©”íƒ€ë°ì´í„°ì— PDF íŒŒì¼ëª…, ì„¹ì…˜ ì œëª©, ì„œë¸Œíƒ€ì´í‹€, ì´ë¯¸ì§€ ëª©ë¡ì„ ì €ì¥
                        metadata = {
                            "pdf_file": pdf_file,
                            "section_title": section_title,
                            "sub_title": sub_title,
                            "image_paths": json.dumps(image_paths)
                        }
                        # Document ê°ì²´ ìƒì„± í›„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                        documents.append(Document(page_content=content_text, metadata=metadata))
            # ìƒì„±ëœ ë¬¸ì„œë¥¼ ë°ì´í„°ë² ì´ìŠ¤(Pinecone)ì— ì¶”ê°€
            database.add_documents(documents)
        # ìƒˆ í•´ì‹œê°’ ì €ì¥ (ì´í›„ ë³€ê²½ ì—¬ë¶€ íŒë‹¨)
        save_hash(hash_file, current_hash)

pinecone_api_key = os.getenv("PINECONE_API_KEY")
pc = Pinecone(api_key=pinecone_api_key)
# ì¸ë±ìŠ¤ ì„ ì •
if index_name not in [idx.name for idx in pc.list_indexes()]:
    pc.create_index(
        name=index_name,
        dimension=3072,
        metric="cosine",
    )
index = pc.Index(index_name)

with st.spinner("ë°ì´í„° ì¸ë±ì‹± ì¤‘..."):
    index_data()


# ==========================================
# lang_graph ì •ì˜
# ==========================================
def retrieve_or_image_node(state: AgentState) -> Command[Literal["retrieve_search", "image_search"]]:
    # ì²« ë²ˆì§¸ ë©”ì‹œì§€ì˜ contentë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    user_query = state["messages"][0]["content"]
    
    # ì´ë¯¸ì§€ URL(ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ)ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬
    match = re.search(IMAGE_PATTERN, user_query, flags=re.IGNORECASE)
    if match:
        image_url = match.group(0)  # ì²« ë²ˆì§¸ ë§¤ì¹­ëœ ì´ë¯¸ì§€ URL ì¶”ì¶œ
        updated_query = re.sub(IMAGE_PATTERN, '', user_query, count=1, flags=re.IGNORECASE).strip()
        state["messages"][0]["content"] = updated_query
        state["image"] = image_url

        return Command(update={'messages': state['messages'], 'image': state['image']}, goto="image_search")
    else:
        return Command(update={'messages': state['messages']}, goto="retrieve_search")

def image_search_node(state: AgentState) -> Command:
    target_image_path = state["image"]
    target_image = Image.open(target_image_path).convert("RGB")

    # ì €ì¥ëœ ì•„ì´ì½˜ ì´ë¯¸ì§€ ì„ë² ë”© ë¡œë”© ë˜ëŠ” ê³„ì‚° í•¨ìˆ˜
    def compute_and_save_embeddings():
        embeddings = {}
        for filename in os.listdir(icon_folder):
            if not filename.lower().endswith((".png", ".jpg", ".jpeg")):
                continue
            image_path = os.path.join(icon_folder, filename)
            # imread_unicodeì™€ resize_imageëŠ” ì‚¬ìš©ìì˜ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.
            img_cv = imread_unicode(image_path)
            if img_cv is None:
                continue
            img_cv = resize_image(img_cv, max_dim=500)
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ CLIP ì „ì²˜ë¦¬)
            img_pil = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
            input_tensor = preprocess(img_pil).unsqueeze(0).to(device)
            with torch.no_grad():
                embedding = model.encode_image(input_tensor)
            embedding /= embedding.norm(dim=-1, keepdim=True)
            embeddings[filename] = embedding.cpu()  # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
        # ì„ë² ë”© ì €ì¥ (pickle ì‚¬ìš©)
        with open(embeddings_path, "wb") as f:
            pickle.dump(embeddings, f)
        
    def load_embeddings():
        if os.path.exists(embeddings_path):
            with open(embeddings_path, "rb") as f:
                embeddings = pickle.load(f)
            return embeddings
        else:
            return None

    # ì €ì¥ëœ ì„ë² ë”©ì´ ì—†ë‹¤ë©´ ê³„ì‚°í•˜ê³  ì €ì¥
    icon_embeddings = load_embeddings()
    if icon_embeddings is None:
        compute_and_save_embeddings()
        icon_embeddings = load_embeddings()

    # íƒ€ê²Ÿ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ì„ë² ë”© ì¶”ì¶œ, ì •ê·œí™”
    target_input = preprocess(target_image).unsqueeze(0).to(device)
    with torch.no_grad():
        target_embedding = model.encode_image(target_input) 
    target_embedding /= target_embedding.norm(dim=-1, keepdim=True)

    # ì €ì¥ëœ ì„ë² ë”©ê³¼ íƒ€ê²Ÿ ì´ë¯¸ì§€ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    results = []
    for filename, embedding in icon_embeddings.items():
        # ê° ì„ë² ë”©ì€ [1, D] í˜•íƒœì´ë¯€ë¡œ ë‚´ì ì„ í†µí•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
        similarity = (target_embedding.cpu() @ embedding.T).item()
        results.append((filename, similarity))

    # ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    results.sort(key=lambda x: x[1], reverse=True)
    state["image"] = results[0][0]

    # stateì— ì¶”ê°€ëœ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ stateë¥¼ ë°˜í™˜
    return Command(update=state)

@tool
def vector_retrieve_tool(query: str) -> List[Document]:
    """Retrieve documents based on the given query."""
    return retriever.invoke(query)

def dynamic_state_modifier(agent_input: AgentState) -> str:
    image_val = agent_input.get("image")
    image_line = f"The Topic of the provided target image is {image_val}. " if image_val and image_val != "no_image" else ""
    
    return (
        f"You are an expert on Mercedes Benz {car_type} car manuals. " +
        image_line +
        "Please consider the information you provided and reply with facts (not opinions). "
        "Translate the answer into Korean and format each item on a separate line. "
        "## {{ì œëª©}}\n"
        "Example: Mercedes Benz EQS: Driver Display Charge Status Window Function\n\n"
        "### {{ì£¼ìš” ì •ë³´}}\n"
        "- Feature Summary and Key Points\n\n"
        "### {{ìƒì„¸ ì„¤ëª…}}\n"
        "- Detailed description of features\n\n"
    )


def retrieve_search_node(state: AgentState) -> Command:
    retrieve_search_agent = create_react_agent(
        llm,
        tools=[vector_retrieve_tool],
        state_modifier=dynamic_state_modifier(state)  # í•¨ìˆ˜ë¡œ êµì²´
    )
    result = retrieve_search_agent.invoke(state)
    # ë‚´ë¶€ retrieval ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
    state["retrieve_result"] = result['messages'][-1].content
    
    # ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ stateì—ì„œ ì¶”ì¶œ (ì²« ë²ˆì§¸ ë©”ì‹œì§€ê°€ HumanMessageë¼ê³  ê°€ì •)
    user_query = state["messages"][0]["content"]
    # retrieverë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ Document ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´
    docs = retriever.invoke(user_query)
    
    displayed_image = None
    for doc in docs:
        meta = doc.metadata
        if "image_paths" in meta and meta["image_paths"]:
            try:
                # image_pathsëŠ” json.dumpsë¡œ ì €ì¥ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë””ì½”ë”©
                image_paths = json.loads(meta["image_paths"])
            except Exception:
                image_paths = meta["image_paths"]
            if isinstance(image_paths, list) and len(image_paths) > 0:
                displayed_image = image_paths[0]
                break
    # retrieval ê²°ê³¼ ì•ì— ì´ë¯¸ì§€ ë§í¬(ì¡´ì¬í•  ê²½ìš°) ì¶”ê°€
    state["retrieve_result"] = (displayed_image or "") + '\n\n' + state["retrieve_result"] + "\n"
    
    # ê¸°ì¡´ messages ë¦¬ìŠ¤íŠ¸ì— retrieval ê²°ê³¼ë¥¼ ì¶”ê°€
    state.setdefault("messages", []).append(
        HumanMessage(content=state["retrieve_result"], name="retrieve_search")
    )
    # ì „ì²´ messages ë¦¬ìŠ¤íŠ¸ë¥¼ ì—…ë°ì´íŠ¸
    return Command(update={'messages': state["messages"]})


def evaluate_node(state: AgentState) -> Command[Literal['web_search', END]]:
    retrieve_result = state.get("retrieve_result", "").strip()

    if retrieve_result == "":
        st.write("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
        return Command(goto='web_search')

    # retrieval ê²°ê³¼ê°€ ì¶©ë¶„í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ LLM í‰ê°€ í”„ë¡¬í”„íŠ¸ ì‹¤í–‰
    eval_prompt = PromptTemplate.from_template(
        "You are an expert on Mercedes Benz " + car_type + " car manuals."
        "Please rate if the retrive results below provide sufficient answers."
        "You must assess that the answers or answers of more than 200 characters are sufficiently consistent with your question."
        "If you don't have enough information to judge, or if you don't provide it in your answer, answer 'yes', and answer no if enough is enough.\n\n"
        "Retrieve Results:\n{result}"
    )
    eval_chain = eval_prompt | llm
    evaluation = eval_chain.invoke({"result": state.get("retrieve_result")})
    if "yes" in evaluation["content"].lower():
          st.write("ë‹µë³€ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
          return Command(goto='web_search')
    else:
          return Command(goto=END)


def web_search_node(state: AgentState) -> Command:
    tavily_search_tool = TavilySearchResults(
        max_results=5,
        search_depth="advanced",
        include_answer=True,
        include_raw_content=True,
        include_images=True,
    )

    web_search_agent = create_react_agent(
        llm, 
        tools=[tavily_search_tool],
        state_modifier = (
            "You are an expert on Mercedes Benz " + car_type + " car manuals."
            "Please reply to the website information in detail. Please translate the answer into Korean and print it out."
            "Please refer to the structure below to organize the website information:\n\n"
            "## {{ì œëª©}}\n"
            "Example: Mercedes Benz EQS: Driver Display Charge Status Window Function\n\n"
            "### {{ì£¼ìš” ì •ë³´}}\n"
            "- Feature Summary and Key Points\n\n"
            "### {{ìƒì„¸ ì„¤ëª…}}\n"
            "- Detailed description of the feature. / {{ì¶œì²˜}}: real website link\n\n"
            "Please translate the answer into Korean, separate each item into separate lines and print it out in a good way."
        )
    )

    result = web_search_agent.invoke(state)
    state["web_result"] = result['messages'][-1].content
    state.setdefault("messages", []).append(
        HumanMessage(content=state["web_result"], name="web_search")
    )
    return Command(update={'messages': state["messages"]})


# ê·¸ë˜í”„ êµ¬ì„±
graph_builder = StateGraph(AgentState)
graph_builder.add_node("retrieve_search", retrieve_search_node)
graph_builder.add_node("evaluate", evaluate_node)
graph_builder.add_node("web_search", web_search_node)
graph_builder.add_node("image_search", image_search_node)
graph_builder.add_node("retrieve_or_image", retrieve_or_image_node)

graph_builder.add_edge(START, "retrieve_or_image")
graph_builder.add_edge("image_search", "retrieve_search")
graph_builder.add_edge("retrieve_search", "evaluate")
graph_builder.add_edge("web_search", END)

graph = graph_builder.compile()

# ==========================================
# Agent ì‹¤í–‰ í•¨ìˆ˜
# ==========================================
def ask_lang_graph_agent(query):
    return graph.invoke({"messages": [{"role": "user", "content": query}]})

# ==========================================
# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬ (í…ìŠ¤íŠ¸, ìŒì„±, ì´ë¯¸ì§€)
# ==========================================
current_key = st.session_state.input_counter

user_prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key=f"user_prompt_{current_key}")
with st.expander("ìŒì„± ì…ë ¥ ë° ì´ë¯¸ì§€ ì²¨ë¶€ ì—´ê¸°"):
    audio_file = st.audio_input("ìŒì„±ì„ ë…¹ìŒí•˜ì„¸ìš”", key=f"audio_file_{current_key}")
    uploaded_image = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["png", "jpg", "jpeg", "gif"], key=f"uploaded_image_{current_key}")

# ìŒì„± ì…ë ¥ì´ ìˆì„ ê²½ìš° ìŒì„± ì¸ì‹
if audio_file is not None:
    with st.spinner("ìŒì„± ì¸ì‹ ì¤‘..."):
        transcript_result = client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            language="ko"
        )
    user_prompt = transcript_result.text

# ì‚¬ìš©ì ì…ë ¥ì´ ìˆì„ ê²½ìš° ì±—ë´‡ ì‘ë‹µ ìƒì„±
if user_prompt:
    combined_prompt = user_prompt or ""
    
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥ (í…ìŠ¤íŠ¸ë§Œ)
    st.session_state.messages.append({"role": "user", "content": combined_prompt})
    if uploaded_image:
        st.image(uploaded_image, width=150, caption="ì²¨ë¶€ëœ ì´ë¯¸ì§€")
        combined_prompt = f"{input_path}/{uploaded_image.name}\n\n{combined_prompt}"
    # lang_graph ê¸°ë°˜ ì±—ë´‡ Agent ì‹¤í–‰
    with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
        assistant_response = ask_lang_graph_agent(combined_prompt)

    assistant_response = assistant_response["messages"][-1].content
    match = re.search(IMAGE_PATTERN, assistant_response, flags=re.IGNORECASE)
    related_image = None
    if match:
        related_image = match.group(0)
        assistant_response = re.sub(IMAGE_PATTERN, '', assistant_response, count=1, flags=re.IGNORECASE)
    
    # TTS ì²˜ë¦¬: ì‘ë‹µì´ ìˆì„ ë•Œë§Œ ì§„í–‰
    if assistant_response.strip():
        try:
            tts = gTTS(assistant_response, lang='ko')
            audio_fp = io.BytesIO()
            tts.write_to_fp(audio_fp)
            audio_fp.seek(0)
            tts_audio_bytes = audio_fp.getvalue()
            # base64 ì¸ì½”ë”©í•˜ì—¬ ì €ì¥ (JSON ì§ë ¬í™” ë¬¸ì œ í•´ê²°)
            tts_audio_b64 = base64.b64encode(tts_audio_bytes).decode("utf-8")
        except Exception as tts_error:
            st.error(f"TTS ì—ëŸ¬: {tts_error}")
            tts_audio_b64 = None
    else:
        tts_audio_b64 = None

    # ì±—ë´‡ ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥ (TTS ë° ê´€ë ¨ ì´ë¯¸ì§€ í¬í•¨)
    assistant_message = {
        "role": "assistant",
        "content": assistant_response,
        "tts": tts_audio_b64,
    }
    if related_image is not None:
        assistant_message["image"] = related_image

    st.session_state.messages.append(assistant_message)
    # ì²˜ë¦¬ í›„ ì¹´ìš´í„° ì¦ê°€ (ë‹¤ìŒ ì§ˆë¬¸ ì‹œ ìƒˆë¡œìš´ ìœ„ì ¯ key ì‚¬ìš©)
    st.session_state.input_counter += 1

# ==========================================
# ì „ì²´ ëŒ€í™” ê¸°ë¡ ì¶œë ¥ (ìµœì¢… ì—…ë°ì´íŠ¸)
# ==========================================
st.markdown("### ëŒ€í™” ê¸°ë¡")
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        # assistant ë©”ì‹œì§€: ê´€ë ¨ ì´ë¯¸ì§€ë¥¼ ë¨¼ì € í‘œì‹œ (ë‹µë³€ ë°”ë¡œ ìœ„ì—)
        if message["role"] == "assistant" and message.get("image"):
            st.markdown(
                f'<img class="responsive-image" src="{message["image"]}" alt="ê´€ë ¨ ì´ë¯¸ì§€">',
                unsafe_allow_html=True
            )

        st.markdown(message["content"])
        # assistant ë©”ì‹œì§€ì— TTSê°€ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ base64 ë””ì½”ë”© í›„ ì˜¤ë””ì˜¤ ì¶œë ¥
        if message["role"] == "assistant" and message.get("tts"):
            audio_bytes = base64.b64decode(message["tts"])
            st.audio(audio_bytes, format="audio/mp3")
